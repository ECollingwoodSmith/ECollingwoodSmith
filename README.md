<!--
**ECollingwoodSmith/ECollingwoodSmith** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->

<!-- Welcome Message -->
<div align="center" style="background-color: #f2f2f2; padding: 20px;">
  <h1 style="color:#2e6da4; font-size: 36px; font-family: Arial, sans-serif;">ðŸ‘‹ Hi there, I'm Elliot!</h1>
</div>

<!-- Introduction -->
<p style="text-align: justify; font-size: 16px; line-height: 1.6; color: #333;">I'm a passionate Data Scientist with a strong background in finance and extensive training in Data Science. My journey began with a solid foundation in Accountancy & Finance, culminating in an MA (Hons) from Heriot-Watt University. Building on this, I completed a rigorous Data Science bootcamp at HyperionDev, where I honed my skills in Python, SQL, machine learning, and data visualization.</p>

**Data Science Expertise:**
<p style="text-align: justify; font-size: 16px; line-height: 1.6; color: #333;">My expertise lies in analysing complex datasets to derive actionable insights and drive decision-making processes. Proficient in Python (including NumPy, Pandas, scikit-learn, spaCy) and SQL, I specialise in machine learning and natural language processing (NLP) tasks. From sentiment analysis to predictive modeling, I thrive on leveraging data to solve real-world challenges.</p>

**Experience at Accelerant Insurance:**
<p style="text-align: justify; font-size: 16px; line-height: 1.6; color: #333;">During my internship at Accelerant Insurance, I delved into Natural Language Processing (NLP) by analyzing insurance industry news articles. Employing web scraping techniques, utilizing HTML for data extraction, I amassed a significant dataset of over 18,000 articles, meticulously parsing their content and metadata. Leveraging powerful libraries like TensorFlow and sklearn, I fine-tuned a pre-trained BERT model tailored specifically for the insurance domain, thereby refining the accuracy of sentiment analysis. The insights derived from this project were visualized using Matplotlib and presented to the Chief Decision Scientist, demonstrating my ability to effectively communicate findings to stakeholders.</p>

**Finance Background:**
<p style="text-align: justify; font-size: 16px; line-height: 1.6; color: #333;">With practical experience gained from internships at Credit Suisse and Grant Thornton, I've acquired a deep understanding of financial markets, financial reporting, and audit procedures. My dissertation on volatility forecasting showcased my ability to apply advanced statistical methods to financial data and conduct predictive modeling, further enriching my analytical toolkit.</p>

**Ongoing Projects:**
<p style="text-align: justify; font-size: 16px; line-height: 1.6; color: #333;">Currently, I'm deeply immersed in hands-on projects exploring the intersection of machine learning, AI, and finance. Drawing from my experience in the insurance sector and recent applications to other firms in the field, my focus is on leveraging libraries like TensorFlow and PyTorch. These projects delve into algorithmic trading strategies and risk management models, aiming to forecast market trends, optimise portfolio allocations, and mitigate financial risks. Through the application of machine learning techniques to both financial and insurance data, my goal is to uncover actionable insights that inform investment decisions and drive innovation and stability in both sectors.</p>

**Active GitHub Contributor:**
<p style="text-align: justify; font-size: 16px; line-height: 1.6; color: #333;">Keen to engage with fellow data enthusiasts, I'm an enthusiastic contributor on GitHub, actively participating within the dynamic community of data scientists. Let's collaborate and create something exceptional together!</p>
